{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed73f32-302c-41fa-aafa-efbf11e62a30",
   "metadata": {},
   "source": [
    "# Find Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e87389-e589-47b1-8a22-65a821f13f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f99d5d-7c25-4720-8c33-1fa3dc21efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"/ocean/projects/mth250011p/shared/215a/final_project/data/raw_text.pkl\"\n",
    "\n",
    "with open(raw_path, \"rb\") as f:\n",
    "    raw = pickle.load(f)\n",
    "\n",
    "print(type(raw))\n",
    "print(list(raw.keys())[:5])  # story ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26208e2b-a01c-42a0-91f0-e9fd828d1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the second story\n",
    "ds2 = raw['thatthingonmyarm']\n",
    "type(ds2)\n",
    "words2 = ds2.data\n",
    "len(words2), words2[:20]\n",
    "text2 = \" \".join(words2)\n",
    "# print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702dc3a-e74e-461c-94eb-9fbedd138239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained BERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13a8d2-f7e3-4ec5-9dfb-1e3e66582efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_word_embeddings_for_words(words,\n",
    "                                       tokenizer,\n",
    "                                       model,\n",
    "                                       device=DEVICE,\n",
    "                                       layer_index=-1,\n",
    "                                       chunk_size=256):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        words: list[str], e.g. ds.data\n",
    "    Returns:\n",
    "        embs: np.ndarray, shape (T, 768), where T = len(words)\n",
    "    Notes:\n",
    "        - Automatically performs chunking to avoid exceeding BERT's max_length=512\n",
    "        - Each word’s embedding is computed as the average of its subword embeddings\n",
    "    \"\"\"\n",
    "    T = len(words)\n",
    "    dim = model.config.hidden_size  # 768\n",
    "    embs = np.zeros((T, dim), dtype=np.float32)\n",
    "    counts = np.zeros(T, dtype=np.int32)\n",
    "\n",
    "    start = 0\n",
    "    while start < T:\n",
    "        end = min(start + chunk_size, T)\n",
    "        chunk_words = words[start:end]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            chunk_words,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc, output_hidden_states=True)\n",
    "            # Select the specified layer (default = last layer)\n",
    "            layer = out.hidden_states[layer_index][0].cpu().numpy() \n",
    "\n",
    "        # Extract mapping from tokenizer tokens --> original words\n",
    "        word_ids = enc.word_ids()\n",
    "\n",
    "        for tok_idx, w_id in enumerate(word_ids):\n",
    "            if w_id is None:\n",
    "                continue\n",
    "            global_w = start + w_id  # Map back to global word index\n",
    "            if 0 <= global_w < T:\n",
    "                embs[global_w] += layer[tok_idx]\n",
    "                counts[global_w] += 1\n",
    "\n",
    "        start = end\n",
    "\n",
    "    # Average over subword embeddings for each word\n",
    "    for i in range(T):\n",
    "        if counts[i] > 0:\n",
    "            embs[i] /= counts[i]\n",
    "\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33feebcf-02f3-43d1-87a4-73762ca5d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = {}  # story_id -> dict\n",
    "\n",
    "for story_id, ds in raw.items():\n",
    "    words = ds.data               # list[str]\n",
    "    word_times = ds.data_times    # np.array, shape (T,)\n",
    "    tr_times = ds.tr_times        # np.array, shape (n_TR,)\n",
    "\n",
    "    print(f\"Processing story: {story_id}, #words = {len(words)}\")\n",
    "\n",
    "    embs = get_bert_word_embeddings_for_words(\n",
    "        words=words,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=DEVICE,\n",
    "        layer_index=-1,    # last layer\n",
    "        chunk_size=256\n",
    "    )  # shape (T, 768)\n",
    "\n",
    "    print(\"  embeddings shape:\", embs.shape)\n",
    "\n",
    "    bert_embeddings[story_id] = {\n",
    "        \"words\": words,\n",
    "        \"word_times\": word_times,\n",
    "        \"tr_times\": tr_times,\n",
    "        \"embeddings\": embs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7794a6-3d0b-45e5-8633-3382a1735a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "out_dir = Path.cwd() / \"embeddings\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_path = out_dir / \"bert_base_uncased_word_embeddings.pkl\"\n",
    "\n",
    "with open(out_path, \"wb\") as f:\n",
    "    pickle.dump(bert_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ff152-b7c6-4fde-b20b-582374223546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check embeddings\n",
    "with open(out_path, \"rb\") as f:\n",
    "    bert_emb = pickle.load(f)\n",
    "\n",
    "print(type(bert_emb))\n",
    "print(list(bert_emb.keys())[:5])\n",
    "story_ids = list(bert_emb.keys())\n",
    "second_id = story_ids[0]\n",
    "second_id\n",
    "item = bert_emb[second_id]\n",
    "\n",
    "words = item[\"words\"]              # list[str]\n",
    "word_times = item[\"word_times\"]    # numpy array, shape (T,)\n",
    "embeddings = item[\"embeddings\"]    # numpy array, shape (T, 768)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"{i:02d}  word = {words[i]!r}\")\n",
    "    print(\"embedding[:10] =\", embeddings[i][:10])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e875626-ac3f-41aa-a302-bef1b80dacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2 = pca.fit_transform(embeddings)\n",
    "\n",
    "norm = (X2[:,0] - X2[:,0].min()) / (X2[:,0].max() - X2[:,0].min())\n",
    "\n",
    "plt.figure(figsize=(5,5), dpi=150)\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "# Scatter plot with transparency + colormap\n",
    "plt.scatter(\n",
    "    X2[:,0], X2[:,1],\n",
    "    c=norm, cmap=\"viridis\",     \n",
    "    s=18,                      \n",
    "    alpha=0.75,                \n",
    "    edgecolor=\"none\"\n",
    ")\n",
    "\n",
    "plt.title(f\"PCA of BERT Embeddings — {second_id}\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"PC 1\", fontsize=14)\n",
    "plt.ylabel(\"PC 2\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "for spine in [\"top\", \"right\"]:\n",
    "    plt.gca().spines[spine].set_visible(False)\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Normalized PC1 value\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2ad49-112f-4571-9693-e5f42e3f591d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9aabc-616f-46ac-b55e-df9daa8c5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import downsample_word_vectors, make_delayed\n",
    "\n",
    "BASE_DIR = Path(\"/ocean/projects/mth250011p/shared/215a/final_project\")\n",
    "TEXT_PATH = BASE_DIR / \"data\" / \"raw_text.pkl\"\n",
    "\n",
    "BOLD_BASE = BASE_DIR / \"data\"\n",
    "SUBJECT_DIRS = {\n",
    "    2: BOLD_BASE / \"subject2\",\n",
    "    3: BOLD_BASE / \"subject3\",\n",
    "}\n",
    "\n",
    "BERT_EMB_PATH = Path.cwd() / \"embeddings\" / \"bert_base_uncased_word_embeddings.pkl\"\n",
    "\n",
    "# DataSequence (wordseqs)\n",
    "with open(TEXT_PATH, \"rb\") as f:\n",
    "    wordseqs = pickle.load(f)   # dict: story_id -> DataSequence\n",
    "\n",
    "print(\"wordseqs stories:\", list(wordseqs.keys())[:5])\n",
    "\n",
    "# BERT embedding\n",
    "with open(BERT_EMB_PATH, \"rb\") as f:\n",
    "    bert_emb = pickle.load(f)   # dict: story_id -> {..., \"embeddings\": (T,768)}\n",
    "\n",
    "print(\"bert_emb stories:\", list(bert_emb.keys())[:5])\n",
    "\n",
    "# check story id\n",
    "stories = sorted(set(wordseqs.keys()) & set(bert_emb.keys()))\n",
    "print(\"num stories:\", len(stories))\n",
    "stories[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8e9fc-a99f-49a7-845c-76b6d93542e6",
   "metadata": {},
   "source": [
    "## downsample from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd9348-2f96-4e8c-8f2d-d7bfa069bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample_word_vectors\n",
    "word_vectors = {}\n",
    "for sid in stories:\n",
    "    embs = bert_emb[sid][\"embeddings\"]   # (num_words, 768)\n",
    "    word_vectors[sid] = embs.astype(\"float32\")\n",
    "\n",
    "downsampled_semanticseqs = downsample_word_vectors(\n",
    "    stories=stories,\n",
    "    word_vectors=word_vectors,\n",
    "    wordseqs=wordseqs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a8ff6-49af-4efa-92f1-5cde44b85e10",
   "metadata": {},
   "source": [
    "## trim and delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048288a-de6a-42d5-8bbf-89914bc457de",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"/ocean/projects/mth250011p/ypan14/preprocessing\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def preprocess_subject_streaming(subject_id, delays=None):\n",
    "    subj_dir = SUBJECT_DIRS[subject_id]\n",
    "    assert subj_dir.is_dir(), f\"{subj_dir} does not exist\"\n",
    "\n",
    "    missing_stories = []\n",
    "\n",
    "    for sid in stories:\n",
    "        ds = wordseqs[sid]\n",
    "        tr_times = ds.tr_times\n",
    "        stim_tr  = downsampled_semanticseqs[sid]\n",
    "\n",
    "        assert stim_tr.shape[0] == len(tr_times)\n",
    "\n",
    "        bold_path = subj_dir / f\"{sid}.npy\"\n",
    "        if not bold_path.is_file():\n",
    "            print(f\"[WARN] Subject {subject_id}: missing BOLD for story '{sid}', skipping.\")\n",
    "            missing_stories.append(sid)\n",
    "            continue\n",
    "\n",
    "        bold = np.load(bold_path)\n",
    "\n",
    "        n_stim = stim_tr.shape[0]\n",
    "        n_bold = bold.shape[0]\n",
    "\n",
    "        if n_stim < n_bold:\n",
    "            print(f\"[WARN] {sid}: stim shorter than bold, skipping.\")\n",
    "            missing_stories.append(sid)\n",
    "            continue\n",
    "\n",
    "        # TR trimming to match BOLD length\n",
    "        diff = n_stim - n_bold\n",
    "        drop_start = diff // 3 if diff > 0 else 0\n",
    "        drop_end = diff - drop_start if diff > 0 else 0\n",
    "\n",
    "        stim_trim = stim_tr[drop_start : n_stim - drop_end]\n",
    "\n",
    "        if stim_trim.shape[0] != n_bold:\n",
    "            print(f\"[WARN] {sid}: mismatch after trim, skipping.\")\n",
    "            missing_stories.append(sid)\n",
    "            continue\n",
    "\n",
    "        # delay\n",
    "        if delays is None:\n",
    "            raise ValueError(\"delays must be provided if only X_delayed is saved.\")\n",
    "\n",
    "        X_delayed = make_delayed(stim_trim, delays=delays)\n",
    "        X_delayed = X_delayed.astype(\"float32\")\n",
    "\n",
    "        bold = bold.astype(\"float32\")\n",
    "\n",
    "        result = {\n",
    "            \"X_delayed\": X_delayed,   # (N, 768 * len(delays))\n",
    "            \"bold\": bold,             # (N, n_vox)\n",
    "        }\n",
    "\n",
    "        out_file = OUT_DIR / f\"subject{subject_id}_{sid}_Xdelayed.pkl\"\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "\n",
    "        print(\n",
    "            f\"[SAVE] Subject {subject_id}, story {sid}: \"\n",
    "            f\"X_delayed {X_delayed.shape}, bold {bold.shape}, saved\"\n",
    "        )\n",
    "\n",
    "        del bold, stim_trim, X_delayed, result\n",
    "\n",
    "    if missing_stories:\n",
    "        print(f\"\\n[INFO] Subject {subject_id} skipped stories:\")\n",
    "        for s in missing_stories:\n",
    "            print(\"  -\", s)\n",
    "    else:\n",
    "        print(f\"\\n[INFO] Subject {subject_id}: all stories processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7557c2-6904-4577-a3a8-45feab8e700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = [1,2,3,4]\n",
    "preprocess_subject_streaming(2, delays=delays)\n",
    "preprocess_subject_streaming(3, delays=delays)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
